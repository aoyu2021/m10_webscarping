# m10_webscarping
# Ao_Yu_800745651

Part I. Create web-scraper to load csv file into S3 Bucket 

M10_webscraper_assignment.ipybn

# Use the webdriver of the Selenium package to simulate the training operation and read the table

•	Step 1. Load the script, M10_webscraper_assignment.ipybn, into your repository as a separate file
•	Step 2. Create an s3 bucket to store your outputs of csv files. Note: I used a bucket named ‘database-update-bucket’.
 
•	Step 3. Update the M10_webscraper_assignment.ipybn file to include your s3 path
•	Step 4. Test the file by inspecting the output on the s3 bucket
•	Step 5. Once the test has passed, commit the changes to the master branch of your GitHub repository
•	Step 6. Note that the output of the file has an issue! The header inserts a blank row! Update the script that was provided to remove the blank row in the csv output file
•	Step 7. Test the file by inspecting the output on the s3 bucket
•	Step 8. Once the test has passed, commit the changes to your script the master branch of your GitHub repository
•	Step 9. Provide evidence that the file and script were successfully updated by providing the link to s3 and the .ipybn file in the repository. 
•	Step 10. Be sure to update your READ_ME.md file to reflect the script actions and ensure proper documentation.

Part II. Update web-scraper to iterate all results and load csv file into S3 Bucket

M10_webscraper_assignment.ipybn
